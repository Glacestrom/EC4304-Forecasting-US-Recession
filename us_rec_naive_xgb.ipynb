{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sktime --ignore-installed llvmlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2014, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file into dataframe\n",
    "us_rates_rec_df = pd.read_csv('us_rates_rec.csv')\n",
    "\n",
    "# Convert `date` column to monthly periods\n",
    "us_rates_rec_df['date'] = pd.to_datetime(us_rates_rec_df['date'], format = '%Y-%m-%d')\n",
    "us_rates_rec_df['month'] = us_rates_rec_df['date'].dt.to_period('M')\n",
    "\n",
    "us_rates_rec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set start and end dates in monthly periods\n",
    "\n",
    "date_series = pd.date_range(start='1953-04-01', end='2000-12-31')\n",
    "date_series = date_series.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables used in models\n",
    "\n",
    "max_lags = 18\n",
    "max_horizon = 12\n",
    "time_start = date_series[0] # + max_lags\n",
    "time_end = date_series[-1]\n",
    "model_vars = ['spr1', 'spr5', 'spr10', 'hyspr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(834, 15)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for data starting from 1953m4\n",
    "\n",
    "mask_start = us_rates_rec_df['month'] >= time_start\n",
    "mask_end = us_rates_rec_df['month'] > time_end\n",
    "\n",
    "us_rates_rec_df = us_rates_rec_df[mask_start].reset_index(drop=True)\n",
    "us_rates_rec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Full Dataset': 0.8645083932853717,\n",
      " 'Test Dataset': 0.89272030651341,\n",
      " 'Train Dataset': 0.8516579406631762}\n"
     ]
    }
   ],
   "source": [
    "# Naive Model 1 Accuracy: Always predict expansion (0)\n",
    "\n",
    "naive_accuracy = {}\n",
    "\n",
    "naive_accuracy['Full Dataset'] = 1 - us_rates_rec_df['rec'].sum()/us_rates_rec_df.shape[0]\n",
    "naive_accuracy['Train Dataset'] = 1 - us_rates_rec_df.loc[~mask_end, 'rec'].sum()/us_rates_rec_df.loc[~mask_end].shape[0]\n",
    "naive_accuracy['Test Dataset'] = 1 - us_rates_rec_df.loc[mask_end, 'rec'].sum()/us_rates_rec_df.loc[mask_end].shape[0]\n",
    "\n",
    "pprint(naive_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lagged variables for rec and ir spreads\n",
    "\n",
    "for lag in range(1, max_lags + max_horizon):\n",
    "    if lag == 12:\n",
    "        model_vars += ['rec']\n",
    "    lagged = us_rates_rec_df[model_vars].shift(lag)\n",
    "    lagged.columns = [x + 'L' + str(lag) for x in model_vars]\n",
    "\n",
    "    us_rates_rec_df = pd.concat((us_rates_rec_df, lagged), axis=1)\n",
    "\n",
    "model_vars.remove('rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec    rec        1.000000\n",
       "       recL1      0.887377\n",
       "spr1   spr1       1.000000\n",
       "       spr1L1     0.901540\n",
       "spr5   spr5       1.000000\n",
       "       spr10      0.953546\n",
       "       spr5L1     0.944415\n",
       "       spr5L2     0.866180\n",
       "       spr5L3     0.801678\n",
       "       spr10L1    0.908335\n",
       "       spr10L2    0.840818\n",
       "spr10  spr5       0.953546\n",
       "       spr10      1.000000\n",
       "       spr5L1     0.906134\n",
       "       spr5L2     0.839114\n",
       "       spr10L1    0.958322\n",
       "       spr10L2    0.896323\n",
       "       spr10L3    0.844189\n",
       "hyspr  hyspr      1.000000\n",
       "       hysprL1    0.968097\n",
       "       hysprL2    0.917090\n",
       "       hysprL3    0.870418\n",
       "       hysprL4    0.827970\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correlation matrix where absolute correlation coefficient > 0.8\n",
    "\n",
    "corr_matrix = us_rates_rec_df.corr()\n",
    "corr_matrix.where(corr_matrix.abs().gt(0.8)).stack()[:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Full Dataset': 0.9735894357743097,\n",
      " 'Test Dataset': 0.9769230769230769,\n",
      " 'Train Dataset': 0.972027972027972}\n"
     ]
    }
   ],
   "source": [
    "# Naive Model 2 Accuracy: rec = recL1\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "naive_accuracy2 = {}\n",
    "\n",
    "mask_start = us_rates_rec_df['month'] >= time_start\n",
    "mask_end = us_rates_rec_df['month'] > time_end\n",
    "\n",
    "naive_pred_full = us_rates_rec_df['recL1'][1:]\n",
    "naive_pred_train = us_rates_rec_df.loc[~mask_end, 'recL1'][1:]\n",
    "naive_pred_test = us_rates_rec_df.loc[mask_end, 'recL1'][1:]\n",
    "\n",
    "naive_accuracy2['Full Dataset'] = accuracy_score(us_rates_rec_df['rec'][1:], naive_pred_full)\n",
    "naive_accuracy2['Train Dataset'] = accuracy_score(us_rates_rec_df.loc[~mask_end, 'rec'][1:], naive_pred_train)\n",
    "naive_accuracy2['Test Dataset'] = accuracy_score(us_rates_rec_df.loc[mask_end, 'rec'][1:], naive_pred_test)\n",
    "\n",
    "pprint(naive_accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(834, 139)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only columns to be used in models\n",
    "\n",
    "us_rates_rec_df = us_rates_rec_df[['month', 'rec', 'spr1', 'spr5', 'spr10', 'hyspr'] \n",
    "    + [f'recL{lag}' for lag in range(12, max_lags+max_horizon)] + [f'{x}L{lag}' for x in model_vars for lag in range(1, max_lags+max_horizon)]]\n",
    "\n",
    "us_rates_rec_df.set_index('month', inplace = True)\n",
    "\n",
    "us_rates_rec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (573, 138), X_test: (261, 138), y_train: (573,), y_test: (261,)\n"
     ]
    }
   ],
   "source": [
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from skopt.space import Real, Integer\n",
    "from sklearn import metrics\n",
    "\n",
    "# Split X and y variables into sequential train and test sets\n",
    "# This means that the last 261 observations constitute the test set\n",
    "X_train, X_test = temporal_train_test_split(us_rates_rec_df.drop('rec', axis=1), test_size=261)\n",
    "y_train, y_test = temporal_train_test_split(us_rates_rec_df['rec'], test_size=261)\n",
    "\n",
    "# Define the parameter grid for cross-validation to search through\n",
    "param_grid = {\n",
    "    'n_estimators': Integer(100, 500),\n",
    "    'max_depth': Integer(2, 6),\n",
    "    'gamma': Real(0, 1.0),\n",
    "    'min_child_weight': Real(0, 10.0),\n",
    "    'subsample': Real(0.5, 1.0),\n",
    "    'colsample_bytree': Real(0.4, 1.0),\n",
    "    'reg_lambda': Real(0, 100),\n",
    "    'scale_pos_weight': Real(1, 6)\n",
    "}\n",
    "\n",
    "# Define (negative) log-loss as the loss function, aka scorer\n",
    "log_loss = metrics.make_scorer(metrics.log_loss, greater_is_better=False, needs_proba=True, labels=[0, 1])\n",
    "\n",
    "print(f'X_train: {X_train.shape}, X_test: {X_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from xgboost import XGBClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sktime.utils.plotting import plot_series\n",
    "import seaborn as sns\n",
    "\n",
    "# Create empty dataframes and dictionaries to store results\n",
    "xgb_train_df = pd.DataFrame(\n",
    "    columns=['Obs', 'TN', 'FN', 'FP', 'TP', 'Accuracy', 'F1 Score', 'PR AUC', 'Precision', 'Sensitivity', 'Specificity', 'ROC AUC']\n",
    ").reindex([f'h{h}' for h in range(1, max_horizon+1)])\n",
    "\n",
    "xgb_test_df = pd.DataFrame(\n",
    "    columns=['Obs', 'TN', 'FN', 'FP', 'TP', 'Accuracy', 'F1 Score', 'PR AUC', 'Precision', 'Sensitivity', 'Specificity', 'ROC AUC']\n",
    ").reindex([f'h{h}' for h in range(1, max_horizon+1)])\n",
    "\n",
    "xgb_pred_df = pd.DataFrame().reindex(us_rates_rec_df.index)\n",
    "xgb_prob_df = pd.DataFrame().reindex(us_rates_rec_df.index)\n",
    "\n",
    "xgb_params = {}\n",
    "\n",
    "xgb_features = {}\n",
    "\n",
    "# Loop through each forecast horizon (in months) from 1 to 12\n",
    "for h in range(1, max_horizon+1):\n",
    "    # Create new filters for each forecast horizon, eg. start date increases by 1 period\n",
    "    time_start2 = time_start + h - 1\n",
    "    p2 = max_lags + h\n",
    "    mask_startX = X_train.index >= time_start2\n",
    "    mask_starty = y_train.index >= time_start2\n",
    "\n",
    "    # Select only X and y columns and observations for the relevant forecast horizon\n",
    "    X_train_h = X_train.loc[mask_startX, [f'recL{lag}' for lag in range(12, p2)] + [f'{x}L{lag}' for x in model_vars for lag in range(h, p2)]]\n",
    "    X_test_h = X_test[[f'recL{lag}' for lag in range(12, p2)] + [f'{x}L{lag}' for x in model_vars for lag in range(h, p2)]]\n",
    "    y_train_h = y_train.loc[mask_starty]\n",
    "\n",
    "    # Create an expanding window splitter for cross-validation\n",
    "    cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    # Create an XGB classifier with a fixed learning rate and objective\n",
    "    # Set the random state to 42 (the meaning of life, the universe, and everything!) for reproducible results\n",
    "    xgb_clf = XGBClassifier(learning_rate=0.01, objective='binary:logistic', random_state=42)\n",
    "\n",
    "    # Create a Bayesian optimization search object with search spaces, scoring and cv as previously defined\n",
    "    # Fit the search object on the train dataset for the respective forecast horizon\n",
    "    xgb_search = BayesSearchCV(estimator=xgb_clf, search_spaces=param_grid, n_iter=100, scoring=log_loss, cv=cv, random_state=42)\n",
    "    xgb_search.fit(X=X_train_h, y=y_train_h)\n",
    "\n",
    "    # Extract the best parameters found\n",
    "    # Fit an XGB classifier with the best parameters on the train set\n",
    "    xgb_params[f'h{h}'] = xgb_search.best_params_\n",
    "    xgb_fitted = XGBClassifier(**xgb_search.best_estimator_.get_params()).fit(X=X_train_h, y=y_train_h)\n",
    "\n",
    "    # Calculate the predicted recession binary values and probabilities for the train set\n",
    "    y_pred_train = xgb_fitted.predict(X=X_train_h)\n",
    "    y_prob_train = xgb_fitted.predict_proba(X_train_h)[:, 1]\n",
    "\n",
    "    # Unravel the confusion matrix to obtain TN, FP, FN and TP values for the train set\n",
    "    tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_train_h, y_pred_train).ravel()\n",
    "\n",
    "    xgb_train_df.loc[f'h{h}', 'Obs'] = len(X_train_h)\n",
    "    xgb_train_df.loc[f'h{h}', 'TN'] = tn_train\n",
    "    xgb_train_df.loc[f'h{h}', 'FN'] = fn_train\n",
    "    xgb_train_df.loc[f'h{h}', 'FP'] = fp_train\n",
    "    xgb_train_df.loc[f'h{h}', 'TP'] = tp_train\n",
    "\n",
    "    # Compute the 7 different evaluation metrics for fitted model on the train set\n",
    "    xgb_train_df.loc[f'h{h}', 'Accuracy'] = accuracy_score(y_train_h, y_pred_train)\n",
    "    xgb_train_df.loc[f'h{h}', 'F1 Score'] = f1_score(y_train_h, y_pred_train)\n",
    "    xgb_train_df.loc[f'h{h}', 'PR AUC'] = average_precision_score(y_train_h, y_prob_train)\n",
    "    xgb_train_df.loc[f'h{h}', 'Precision'] = precision_score(y_train_h, y_pred_train)\n",
    "    xgb_train_df.loc[f'h{h}', 'Sensitivity'] = recall_score(y_train_h, y_pred_train)\n",
    "    xgb_train_df.loc[f'h{h}', 'Specificity'] = recall_score(y_train_h, y_pred_train, pos_label=0)\n",
    "    xgb_train_df.loc[f'h{h}', 'ROC AUC'] = roc_auc_score(y_train_h, y_prob_train)\n",
    "\n",
    "    # Calculate the predicted recession binary values and probabilities for the test set\n",
    "    y_pred_test = xgb_fitted.predict(X=X_test_h)\n",
    "    y_prob_test = xgb_fitted.predict_proba(X_test_h)[:, 1]\n",
    "    \n",
    "    # Unravel the confusion matrix to obtain TN, FP, FN and TP values for the test set\n",
    "    tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "\n",
    "    xgb_test_df.loc[f'h{h}', 'Obs'] = len(X_test_h)\n",
    "    xgb_test_df.loc[f'h{h}', 'TN'] = tn_test\n",
    "    xgb_test_df.loc[f'h{h}', 'FN'] = fn_test\n",
    "    xgb_test_df.loc[f'h{h}', 'FP'] = fp_test\n",
    "    xgb_test_df.loc[f'h{h}', 'TP'] = tp_test\n",
    "\n",
    "    # Compute the 7 different evaluation metrics for fitted model on the test set\n",
    "    xgb_test_df.loc[f'h{h}', 'Accuracy'] = accuracy_score(y_test, y_pred_test)\n",
    "    xgb_test_df.loc[f'h{h}', 'F1 Score'] = f1_score(y_test, y_pred_test)\n",
    "    xgb_test_df.loc[f'h{h}', 'PR AUC'] = average_precision_score(y_test, y_prob_test)\n",
    "    xgb_test_df.loc[f'h{h}', 'Precision'] = precision_score(y_test, y_pred_test)\n",
    "    xgb_test_df.loc[f'h{h}', 'Sensitivity'] = recall_score(y_test, y_pred_test)\n",
    "    xgb_test_df.loc[f'h{h}', 'Specificity'] = recall_score(y_test, y_pred_test, pos_label=0)\n",
    "    xgb_test_df.loc[f'h{h}', 'ROC AUC'] = roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "    # Extract the top 20 features by importance\n",
    "    feature_importance = pd.Series(xgb_fitted.feature_importances_, index=X_train_h.columns)\n",
    "    best_features = feature_importance.sort_values(ascending=False).head(20)\n",
    "    xgb_features[f'h{h}'] = best_features\n",
    "\n",
    "    # Convert np arrays to datetime series for time series plotting\n",
    "    y_pred = pd.concat([pd.Series(y_pred_train).set_axis(y_train_h.index), pd.Series(y_pred_test).set_axis(y_test.index)])\n",
    "    y_prob = pd.concat([pd.Series(y_prob_train).set_axis(y_train_h.index), pd.Series(y_prob_test).set_axis(y_test.index)])\n",
    "\n",
    "    xgb_pred_df[f'h{h}'] = y_pred\n",
    "    xgb_prob_df[f'h{h}'] = y_prob\n",
    "\n",
    "    # Plot the actual recession binary values with the predicted binary values\n",
    "    plot_series(y_train_h, y_test, y_pred, labels=['y_train', 'y_test', 'y_pred'], x_label='Date', y_label='Rec(Binary)')\n",
    "    plt.title(f'us_recbinary_xgb_h{h}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'us_recbinary_xgb_h{h}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot the actual recession binary values with the predicted probabilities\n",
    "    plot_series(y_train_h, y_test, y_prob, labels=['y_train', 'y_test', 'y_prob'], x_label='Date', y_label='Pr(Rec)')\n",
    "    plt.title(f'us_recprob_xgb_h{h}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'us_recprob_xgb_h{h}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot the top 20 features by importance\n",
    "    # Save all three graphs and close them for matplotlib to release memory\n",
    "    plt.figure(figsize=(20,28))\n",
    "    sns.barplot(x=best_features.values, y=best_features.index)\n",
    "    plt.title(f'feature_importance_xgb_h{h}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'feature_importance_xgb_h{h}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results and estimates to CSV files\n",
    "\n",
    "xgb_train_df.to_csv('rec_xgb_train_results.csv')\n",
    "xgb_test_df.to_csv('rec_xgb_test_results.csv')\n",
    "xgb_pred_df.to_csv('rec_xgb_pred_estimates.csv')\n",
    "xgb_prob_df.to_csv('rec_xgb_prob_estimates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some manipulation and export feature importances to CSV\n",
    "\n",
    "tmp_feature = xgb_features['h1'].rename_axis('h1_feature').reset_index(name='h1_importance')\n",
    "feature_importance_df = tmp_feature.T\n",
    "\n",
    "for h in range(2, max_horizon+1):\n",
    "    tmp_feature = xgb_features[f'h{h}'].rename_axis(f'h{h}_feature').reset_index(name=f'h{h}_importance')\n",
    "    feature_importance_df = pd.concat([feature_importance_df, tmp_feature.T])\n",
    "\n",
    "feature_importance_df.to_csv('rec_xgb_feature_importances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>subsample</th>\n",
       "      <th>scale_pos_weight</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>gamma</th>\n",
       "      <th>colsample_bytree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>h1</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h2</th>\n",
       "      <td>0.873563</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h3</th>\n",
       "      <td>0.885057</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h4</th>\n",
       "      <td>0.881226</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h5</th>\n",
       "      <td>0.873563</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h6</th>\n",
       "      <td>0.877395</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h7</th>\n",
       "      <td>0.885057</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h8</th>\n",
       "      <td>0.881226</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h9</th>\n",
       "      <td>0.885057</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h10</th>\n",
       "      <td>0.869732</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h11</th>\n",
       "      <td>0.869732</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h12</th>\n",
       "      <td>0.873563</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     accuracy  subsample  scale_pos_weight  n_estimators  min_child_weight  \\\n",
       "h1   0.888889        0.8                 1           100              10.0   \n",
       "h2   0.873563        0.7                 1           100              10.0   \n",
       "h3   0.885057        0.8                 1           100              10.0   \n",
       "h4   0.881226        0.7                 1           100              10.0   \n",
       "h5   0.873563        0.8                 1           100              10.0   \n",
       "h6   0.877395        0.7                 1           100              10.0   \n",
       "h7   0.885057        0.7                 1           100              10.0   \n",
       "h8   0.881226        0.7                 1           100              10.0   \n",
       "h9   0.885057        0.7                 1           100              10.0   \n",
       "h10  0.869732        0.7                 1           100               7.0   \n",
       "h11  0.869732        0.7                 1           100               7.0   \n",
       "h12  0.873563        0.7                 1           100              10.0   \n",
       "\n",
       "     max_depth  gamma  colsample_bytree  \n",
       "h1           5    0.4               0.6  \n",
       "h2           6    0.6               0.7  \n",
       "h3           5    0.4               0.6  \n",
       "h4           6    0.6               0.7  \n",
       "h5           5    0.4               0.6  \n",
       "h6           6    0.6               0.7  \n",
       "h7           6    0.6               0.7  \n",
       "h8           6    0.6               0.7  \n",
       "h9           6    0.6               0.7  \n",
       "h10          2    0.4               0.4  \n",
       "h11          2    0.4               0.4  \n",
       "h12          6    0.6               0.7  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform some manipulation and export best parameters to CSV\n",
    "# This can be directly imported and used to reproduce results in future\n",
    "\n",
    "xgb_params_df = pd.DataFrame.from_dict(xgb_params, orient='index')\n",
    "xgb_params_df = xgb_params_df.reindex([f'h{h}' for h in range(1, max_horizon+1)])\n",
    "\n",
    "xgb_params_df.to_csv('rec_xgb_parameters.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b020d37e2d2792244d38dc749b48635155a26318d10fe99dbb63be9f4ff243c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
